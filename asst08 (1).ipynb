{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguist 278: Programming for Linguists<br />\n",
    "Stanford Linguistics, Fall 2020<br />\n",
    "Christopher Potts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8\n",
    "\n",
    "Distributed 2020-11-09<br />\n",
    "Due 2020-11-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit a modified version of this file with the questions completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleTokenizer [2 points]\n",
    "\n",
    "For assignment 2, question 1, you wrote a function called `simple_tokenize` for tokenizing text. This question asks you to convert that function to a method on a `SimpleTokenizer` class.  The function should use the same core logic as `simple_tokenize`, but should also honor the optional class parameter `lower` allowing the user to decide whether to downcase all of the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "\n",
    "    PUNCT = string.punctuation\n",
    "\n",
    "    def __init__(self, lower=True):\n",
    "        self.lower = lower\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"Break str `s` into a list of str.\n",
    "\n",
    "        1. `s` has all of its peripheral whitespace removed.\n",
    "        2. `s` is downcased if `self.lower` is True, otherwise not.\n",
    "        3. `s` is split on whitespace.\n",
    "        4. For each token, any peripheral punctuation on it is stripped\n",
    "           off. Punctuation is here defined by `string.punctuation`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : str\n",
    "            The string to tokenize.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            list of str\n",
    "        \"\"\"\n",
    "        lst = []\n",
    "        s = s.strip()\n",
    "        if self.lower == True:\n",
    "          s = s.lower()\n",
    "        s = s.split(\" \")\n",
    "        for elem in s:\n",
    "          elem = elem.strip(self.PUNCT)\n",
    "          lst.append(elem)\n",
    "        return lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_tokenizer():\n",
    "    lower = True\n",
    "    no_lower = False\n",
    "    examples = [\n",
    "        [\"The dog barked.\", [\"the\", \"dog\", \"barked\"], lower],\n",
    "        [\"The dog barked.\", [\"The\", \"dog\", \"barked\"], no_lower],\n",
    "        ['\"Hello?\", she said.', [\"hello\", \"she\", \"said\"], lower],\n",
    "        [\"A non-issue.\", [\"a\", \"non-issue\"], lower]\n",
    "    ]\n",
    "    err_count = 0\n",
    "    for x, expected, lower in examples:\n",
    "        tokenizer = SimpleTokenizer(lower)\n",
    "        result = tokenizer.tokenize(x)\n",
    "        if result != expected:\n",
    "            print('simple_tokenize error for \"{}\":\\n\\tGot: {}\\n\\tExpected: {}'.format(\n",
    "                x, result, expected))\n",
    "            err_count += 1\n",
    "    print(\"test_simple_tokenize completed with {} errors\".format(err_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_simple_tokenize completed with 0 errors\n"
     ]
    }
   ],
   "source": [
    "test_simple_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age of Acqusition class [2 points]\n",
    "\n",
    "The hackathon introduced the [Age-of-acquisition ratings for 30 thousand English words](https://www.humanities.mcmaster.ca/~vickup/Kuperman-BRM-2012.pdf) (Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert, *Behavior Research Methods*, 2014), which has the following columns:\n",
    "\n",
    "0. `Word`: The word (str)\n",
    "1. `OccurTotal`: token count in their data\n",
    "2. `OccurNum`: Participants who gave an age-of-acquisition, rather than saying \"Unknown\"\n",
    "3. `Rating.Mean`: mean age of aquisition in years of age\n",
    "4. `Rating.SD`: standard deviation of the distribution of ages of acquisition\n",
    "\n",
    "Complete following class definition according to the specifications given by the docstrings and other comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AoA:\n",
    "    def __init__(self, filename_or_url):\n",
    "        \"\"\"Class for working with the Age-of-Acquisition (AoA) dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename_or_url : str\n",
    "            Full path to the file on the local machine or\n",
    "            a URL pointing to the file.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            The dataset as read in by pandas, with \"Word\" as the index.\n",
    "        word_set : set\n",
    "            The set of words in the dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        self.filename_or_url = filename_or_url\n",
    "\n",
    "        # Complete this so that the spreadsheet is read in as a `pd.DataFrame`\n",
    "        # with the 'Word' column providing the index:\n",
    "        \n",
    "        self.df = pd.read_csv(self.filename_or_url, index_col = \"Word\")\n",
    "\n",
    "        # Complete this so that it is defined as the set of words in the dataset:\n",
    "        # the set in the index of `self.df`.\n",
    "        ## TO BE COMPLETED ##\n",
    "        self.word_set = set(self.df.index)\n",
    "\n",
    "\n",
    "    def mean_rating_mean(self, word_or_word_list):\n",
    "        \"\"\"Return the mean of the \"Rating.Mean\" values of the word or\n",
    "        words given in `word_or_word_list`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        word_or_word_list : str or list of str\n",
    "            The words to look-up. All the words provided can be assumed to be\n",
    "            in the dataset.\n",
    "        \"\"\"\n",
    "        return self.df.loc[word_or_word_list][\"Rating.Mean\"].mean()\n",
    "\n",
    "\n",
    "    def foo(self, filename_or_url):\n",
    "        \"\"\"Think of something original for this method, and rename it so that\n",
    "        it matches what it does. Your function can have as many required and\n",
    "        optional arguments as you wish (including none), and it can do whatever\n",
    "        you like withe the dataset. No need to get carried away; I am assuming\n",
    "        it will have about the complexity of `mean_rating_mean`.\n",
    "        \"\"\"\n",
    "        return self.df.loc[\"Rating.SD\"][\"Rating.Mean\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"http://web.stanford.edu/class/linguist278/data/hackathon/Kuperman-BRM-data-2012.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_aoa():\n",
    "    aoa = AoA(\"http://web.stanford.edu/class/linguist278/data/hackathon/Kuperman-BRM-data-2012.csv\")\n",
    "    err_count = 0\n",
    "    if not hasattr(aoa, \"df\"):\n",
    "        print(\"The AoA class should have an attribute `df`.\")\n",
    "        err_count += 1\n",
    "    elif not isinstance(aoa.df, pd.DataFrame):\n",
    "        print(\"The type of the `df` attribute should be `pd.DataFrame`.\")\n",
    "        err_count += 1\n",
    "    if not hasattr(aoa, \"word_set\"):\n",
    "        print(\"The AoA class should have an attribute `word_set`.\")\n",
    "        err_count += 1\n",
    "    elif not isinstance(aoa.word_set, set):\n",
    "        print(\"The type of the `word_set` attribute should be `set`.\")\n",
    "        err_count += 1\n",
    "    examples = ['dog', 'canine']\n",
    "    result = aoa.mean_rating_mean(examples)\n",
    "    expected = 5.625\n",
    "    if result != expected:\n",
    "        print(\"Error for `mean_rating_mean`: for {}, expected {} but got {}\".format(\n",
    "            examples, expected, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the `df` attribute should be `pd.DataFrame`.\n",
      "The type of the `word_set` attribute should be `set`.\n",
      "Error for `mean_rating_mean`: for ['dog', 'canine'], expected 5.625 but got None\n"
     ]
    }
   ],
   "source": [
    "test_aoa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown corpus reader [6 points]\n",
    "\n",
    "This question focuses on building two basic Python classes for processing and working with [the Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus), which is a famous early part-of-speech tagged corpus. Please download the corpus from here:\n",
    "\n",
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown.zip\n",
    "\n",
    "The corpus consists of 500 text files. Each file contains a list of sentences. Each sentence is given on its own line, with no linebreaks. (The files also have a lot of blank lines reflecting passage structure that we will ignore.)\n",
    "\n",
    "The sentences themselves look like this:\n",
    "\n",
    "> Implementation/nn of/in Georgia's/np$ automobile/nn title/nn law/nn was/bedz also/rb recommended/vbn by/in the/at outgoing/jj jury/nn ./.\n",
    "\n",
    "The words have part-of-speech (POS) tags on them. The separator is a forward slash, `/`. So for example, `of/in` is the word \"of\" tagged as a preposition (tag `in`). You don't really need to know what the tags mean for this assignment, but you can find a full glossary of them [here](https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used).\n",
    "\n",
    "For this question, you just need to complete the classes for `BrownCorpus` and `BrownSentence` according to the docstrings. \n",
    "\n",
    "The intuitive idea is that a `BrownCorpus` instance can read in the corpus files and turn the (non-blank) lines in those files into `BrownSentence` instances, where a `BrownSentence` instance is a list of (word, tag) pairs.\n",
    "\n",
    "Below the class definitions, I've included some `assert` statements that you can use to test your code. Feel free to modify these (e.g., by having them print useful messages). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'key': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 1, 'key2': 2, 'key3': {'key3_1': 0}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['key2'] = 2\n",
    "d['key3'] = {'key3_1' : 0}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownCorpus:\n",
    "    def __init__(self, src_dirname=\"brown\"):\n",
    "        \"\"\"This init method should create two attributes:\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        self.src_dirname : str\n",
    "            The `src_dirname` argument stored as an attribute.\n",
    "\n",
    "        self.src_filenames : list of str\n",
    "            The full list of corpus filenames. You can get these by\n",
    "            using `glob.glob` on `self.src_dirname`. Note: the corpus\n",
    "            directory contains a few extra metadata files that need\n",
    "            to be filtered out. Hint: all and only the true corpus\n",
    "            filenames end in a digit.\n",
    "\n",
    "        \"\"\"\n",
    "        self.src_dirname = src_dirname\n",
    "        self.src_filenames = glob.glob(self.src_dirname+'/*[0-9]') \n",
    "\n",
    "\n",
    "    def iter_sentences(self):\n",
    "        \"\"\"This is a method for iterating over the corpus sentences.\n",
    "        It should loop over all the files in `self.src_filenames`, open\n",
    "        each one, iterate through its lines, and turn all the non-blank\n",
    "        lines into `BrownSentence` instances, which it should yield.\n",
    "        Note: `BrownSentence` instances are initializde with a str\n",
    "        and the filename that tracks their origin.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        BrownSentence\n",
    "        \"\"\"\n",
    "        lst = []\n",
    "        for filename in self.src_filenames:\n",
    "            with open (filename,'r') as f:\n",
    "                for line in f:\n",
    "                    if line !='\\n':\n",
    "                        sentence = BrownSentence(line, filename)\n",
    "                        lst.append(sentence)\n",
    "        return lst\n",
    "           \n",
    "                    \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def get_pos_distributions(self):\n",
    "        \"\"\"This method returns a two-dimensional count dict mapping\n",
    "        words to dicts mapping tags to counts. The idea is that this\n",
    "        makes it easy to see how many different POS tags a word\n",
    "        appears with, to get its most or least common tag, etc.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict mapping str to dicts mapping str to int\n",
    "\n",
    "        \"\"\"\n",
    "        ## TO BE COMPLETED ##\n",
    "    \n",
    "        d = {}\n",
    "        sentences = self.iter_sentences()\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence.words)):\n",
    "                word = sentence.words[i]\n",
    "                tag = sentence.tags[i]\n",
    "                if word in d:\n",
    "                    if tag in d[word]:\n",
    "                        d[word][tag]+=1\n",
    "                    else:\n",
    "                        d[word][tag]=1\n",
    "                else:\n",
    "                    d[word] = {tag: 1}\n",
    "        return d\n",
    "\n",
    "class BrownSentence:\n",
    "    def __init__(self, raw_string, src_filename):\n",
    "        \"\"\"This init method should create four attributes:\n",
    "    \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.raw_string : str\n",
    "            Identical to the `raw_string` argument. The presumption\n",
    "            of this class is that `raw_string` is a non-blank line\n",
    "            from a Brown corpus file.\n",
    "\n",
    "        self.src_filename : str\n",
    "            The filename of the file that contains this sentence.\n",
    "            Identical to the argument `src_filename`.\n",
    "\n",
    "        self.lemmas : list of tuple\n",
    "            Created by the method `get_lemmas` defined below.\n",
    "\n",
    "        self.words : list of str\n",
    "            Derived from `self.lemmas`, as the list of the first\n",
    "            members of those tuples. For instance, if\n",
    "\n",
    "            self.lemmas = [('the', 'at', 'cat', 'nn', 'sat', 'vbd')]\n",
    "\n",
    "            then\n",
    "\n",
    "            self.words == ['the', 'cat', 'sat']\n",
    "            \n",
    "\n",
    "        self.tags : list of str\n",
    "            Derived from `self.lemmas`, as the list of the second\n",
    "            members of those tuples. For instance, if\n",
    "\n",
    "            self.lemmas = [('the', 'at', 'cat', 'nn', 'sat', 'vbd')]\n",
    "\n",
    "            then\n",
    "\n",
    "            self.words == ['at', 'nn', 'vbd']\n",
    "\n",
    "        \"\"\"\n",
    "        ## TO BE COMPLETED ##\n",
    "        \n",
    "        self.raw_string = raw_string\n",
    "        self.src_filename = src_filename\n",
    "        self.lemmas = self.get_lemmas()\n",
    "        \n",
    "        self.tags = []\n",
    "        self.words = []\n",
    "        for lemma in self.lemmas:\n",
    "            self.words.append(lemma[0])\n",
    "            self.tags.append(lemma[1])\n",
    "        \n",
    "            \n",
    "        \n",
    "\n",
    "    def get_lemmas(self):\n",
    "        \"\"\"This method creates a list of lemmas from `self.raw_string`.\n",
    "        A lemma is a pair consisting of a word token and a\n",
    "        part-of-speech (POS) tag.  In the Brown corpus, the lemmas of\n",
    "        a sentence are separated by whitespace, and the word and POS\n",
    "        of each lemma as separated by a /. If a lemma contains multiple\n",
    "        / characters, it will be the rightmost one that separates the\n",
    "        word from the POS tag. For example,\n",
    "\n",
    "        \"2-1/2/cd\"\n",
    "\n",
    "        should be parsed as ('2-1/2', 'cd'). Check out the str method\n",
    "        `rsplit` for help with this.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of tuple\n",
    "\n",
    "        \"\"\"\n",
    "        ## TO BE COMPLETED ##\n",
    "        words = self.raw_string.strip().split(' ')\n",
    "        lst = []\n",
    "        for word in words:\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            lst.append(tuple(word.rsplit(\"/\", maxsplit = 1)))\n",
    "        return lst\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Defined as len(self.lemmas)\"\"\"\n",
    "        return len(self.lemmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = BrownSentence(\"  the/at  cat/nn ate/vbd  and/or/cc  slept/vbd ./.\", \"foo.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'at'),\n",
       " ('cat', 'nn'),\n",
       " ('ate', 'vbd'),\n",
       " ('and/or', 'cc'),\n",
       " ('slept', 'vbd'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'ate', 'and/or', 'slept', '.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at', 'nn', 'vbd', 'cc', 'vbd', '.']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent.raw_string.strip() == \"the/at  cat/nn ate/vbd  and/or/cc  slept/vbd ./.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent.src_filename == \"foo.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent.lemmas == [('the', 'at'), ('cat', 'nn'), ('ate', 'vbd'),\n",
    "                       ('and/or', 'cc'), ('slept', 'vbd'), ('.', '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent.words == ['the', 'cat', 'ate', 'and/or', 'slept', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sent.tags == ['at', 'nn', 'vbd', 'cc', 'vbd', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(sent) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code assumes the corpus is in a directory called \"brown\"\n",
    "# in the same directory as this notebook. Feel free to put it\n",
    "# somewhere else if you prefer.\n",
    "\n",
    "corpus = BrownCorpus(src_dirname=\"brown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus.src_filenames) == 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = corpus.get_pos_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dist['commented'] == {'vbd': 16, 'vbn': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dist['!'] == {'.': 1590, '.-hl': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
